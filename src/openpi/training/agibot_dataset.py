import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset
import logging

from .action import AgibotActionState

import decord
from decord import VideoReader, cpu
decord.bridge.set_bridge('torch')

class AgiBotDataset(Dataset):
    def __init__(
        self,
        root_dir: str,
        action_horizon: int = 30,
        index_filename: str = "episodic_dataset_fixed.npy",
        stats_filename: str = "dataset_stats_32dim.json",
        meta_filename: str = "actions_gaussian/meta_data.json",
        normalization: bool = True,
    ):
        self.root_dir = root_dir
        self.action_horizon = action_horizon
        self.normalization = normalization
        self.CHUNK_SIZE = 500

        self.JOINT_DIM = 14
        self.TOTAL_OUTPUT_DIM = 32
        
        env_index_file = os.getenv("AGIBOT_INDEX_FILE")
        if env_index_file:
            logging.info(f"ğŸš© [AgiBotDataset] Overriding index filename from ENV: {env_index_file}")
            index_filename = env_index_file
        # =========================================================

        # 1. åŠ è½½ç´¢å¼•
        index_path = os.path.join(root_dir, index_filename)
        if not os.path.exists(index_path):
            index_path = os.path.join(root_dir, "episodic_dataset_fixed.npy")
        
        logging.info(f"[AgiBotDataset] Loading index from: {index_path}")
        meta_data = np.load(index_path, allow_pickle=True).item()
        
        self.video_paths = meta_data["video_path"]
        self.instructions = meta_data["instructions"]
        self.start_end = meta_data["start_end"]
        
        self.episode_lengths = self.start_end[:, 1] - self.start_end[:, 0]
        self.cumulative_lengths = np.cumsum(self.episode_lengths)
        self.total_frames = self.cumulative_lengths[-1]

        # =========================================================
        # [ä¼˜åŒ–] O(log N) -> O(1) çº¿æ€§æ˜ å°„
        # 2.1äº¿ä¸ª uint32 å ç”¨çº¦ 804MB å†…å­˜
        # =========================================================
        logging.info(f"ğŸš€ Building O(1) frame mapping for {self.total_frames} frames...")
        
        # ä½¿ç”¨ uint32 è¶³å¤Ÿå­˜å‚¨ 76ä¸‡ä¸ª ep_idx (æœ€å¤§æ”¯æŒ 42äº¿)
        self.frame_to_ep = np.zeros(self.total_frames, dtype=np.uint32)
        
        # å¡«å……æ˜ å°„è¡¨
        start_idx = 0
        for ep_idx, length in enumerate(self.episode_lengths):
            end_idx = start_idx + length
            self.frame_to_ep[start_idx : end_idx] = ep_idx
            start_idx = end_idx
            
        logging.info("âœ… O(1) mapping built successfully.")
        # =========================================================

        # print("total_frames in dataset:", self.total_frames)
        
        # 2. åŠ è½½å…ƒæ•°æ® (è·å– dim_list å’Œ total_length)
        meta_json_path = os.path.join(root_dir, meta_filename)
        logging.info(f"[AgiBotDataset] Loading metadata from: {meta_json_path}")
        with open(meta_json_path, "r") as f:
            self.dims_meta = json.load(f)

        # print("dims_meta keys:", list(self.dims_meta.keys())[:10])
        # print("len dims_meta:", len(self.dims_meta))

        # 3. åŠ è½½ç»Ÿè®¡æ•°æ® [å…³é”®ä¿®æ”¹]
        if self.normalization:
            # åˆå§‹åŒ–é»˜è®¤å€¼ (ä»¥é˜²åŠ è½½å¤±è´¥)
            self.state_mean = torch.zeros(32)
            self.state_std = torch.ones(32)
            self.action_mean = torch.zeros(32)
            self.action_std = torch.ones(32)
            
            stats_path = os.getenv("NORM_STATS_FILE")
            
            # --- [Modification 1: Print the EXACT path being used] ---
            logging.info(f"ğŸ” [AgiBotDataset] ------------------------------------------------")
            logging.info(f"ğŸ” [AgiBotDataset] Loading stats from: {stats_path}")
            # ---------------------------------------------------------

            if not stats_path:
                error_msg = "âŒ [AgiBotDataset] Environment variable 'NORM_STATS_FILE' is NOT set!"
                logging.error(error_msg)
                raise ValueError(error_msg)
            
            logging.info(f"ğŸ” [AgiBotDataset] Loading stats from: {stats_path}")
            if not os.path.exists(stats_path):
                raise FileNotFoundError(f"Stats file not found: {stats_path}")

            try:
                with open(stats_path, 'r') as f:
                    full_stats = json.load(f)
                
                if "norm_stats" in full_stats:
                    stats = full_stats["norm_stats"]
                else:
                    stats = full_stats 

                # --- A. Load STATE Stats ---
                if "state" in stats:
                    if "q01" in stats["state"] and "q99" in stats["state"]:
                        self.state_q01 = torch.tensor(stats["state"]["q01"], dtype=torch.float32)
                        self.state_q99 = torch.tensor(stats["state"]["q99"], dtype=torch.float32)
                        
                        # --- [Modification 2: Print State Values] ---
                        logging.info(f"âœ… [AgiBotDataset] Loaded STATE q01/q99")
                        logging.info(f"   --> State q01 (first 5 dims): {self.state_q01[:5].tolist()}")
                        logging.info(f"   --> State q99 (first 5 dims): {self.state_q99[:5].tolist()}")
                        # --------------------------------------------
                    else:
                         logging.warning("âš ï¸ [AgiBotDataset] 'q01/q99' missing in state stats.")

                # --- B. Load ACTION Stats ---
                if "actions" in stats:
                     if "q01" in stats["actions"] and "q99" in stats["actions"]:
                        self.action_q01 = torch.tensor(stats["actions"]["q01"], dtype=torch.float32)
                        self.action_q99 = torch.tensor(stats["actions"]["q99"], dtype=torch.float32)
                        
                        # --- [Modification 3: Print Action Values] ---
                        logging.info(f"âœ… [AgiBotDataset] Loaded ACTION q01/q99")
                        logging.info(f"   --> Action q01 (first 5 dims): {self.action_q01[:5].tolist()}")
                        logging.info(f"   --> Action q99 (first 5 dims): {self.action_q99[:5].tolist()}")
                        # ---------------------------------------------
                     else:
                        logging.warning("âš ï¸ [AgiBotDataset] 'q01/q99' missing in action stats.")
                
                logging.info(f"ğŸ” [AgiBotDataset] ------------------------------------------------")
                    
            except Exception as e:
                logging.error(f"âŒ [AgiBotDataset] JSON Decode Error in {stats_path}: {e}")
                raise e

            # while True:
            #     pass

    def __len__(self):
        return self.total_frames

    # def _get_info_by_idx(self, global_idx): 
    #     ep_idx = np.searchsorted(self.cumulative_lengths, global_idx, side='right')

    #     if ep_idx == 0:
    #         frame_idx_in_ep = global_idx
    #     else:
    #         frame_idx_in_ep = global_idx - self.cumulative_lengths[ep_idx - 1]
    #     return ep_idx, frame_idx_in_ep
    # O(log N) -> O(1) ä¼˜åŒ–ç‰ˆæœ¬

    def _get_info_by_idx(self, global_idx):
        # O(1) ç›´æ¥æŸ¥è¡¨è·å–æ‰€åœ¨çš„ Episode ç´¢å¼•
        ep_idx = self.frame_to_ep[global_idx]

        # è®¡ç®—ç›¸å¯¹å¸§ç´¢å¼•
        if ep_idx == 0:
            frame_idx_in_ep = global_idx
        else:
            # ä»ç„¶éœ€è¦ cumulative_lengths æ¥å¿«é€Ÿè·å–å‰ä¸€ä¸ª episode çš„ç»“æŸä½ç½®
            frame_idx_in_ep = global_idx - self.cumulative_lengths[ep_idx - 1]
            
        return int(ep_idx), int(frame_idx_in_ep)

    def _load_video_frame(self, video_folder, view_prefix, abs_frame_idx):
            chunk_idx = abs_frame_idx // self.CHUNK_SIZE
            local_idx = abs_frame_idx % self.CHUNK_SIZE
            video_name = f"{view_prefix}_{chunk_idx}.mp4"
            video_path = os.path.join(video_folder, video_name)
            
            # [Debug] æ£€æŸ¥è·¯å¾„
            if not os.path.exists(video_path):
                # print(f"âŒ [DEBUG] File NOT Found: {video_path}")
                return torch.zeros((3, 224, 224), dtype=torch.float32)
                
            try:
                # width=224, height=224 è®© decord è‡ªåŠ¨ç¼©æ”¾
                # vr = VideoReader(video_path, ctx=cpu(0), width=224, height=224)
                vr = VideoReader(video_path, ctx=cpu(0))
                idx = min(local_idx, len(vr) - 1)
                
                tensor_img = vr[idx] 
                
                # âŒ [åˆ é™¤è¿™è¡Œ] return tensor_img.permute(2, 0, 1).float() / 255.0
                
                # âœ… [æ”¹ä¸ºè¿™è¡Œ] ä¿æŒ (H, W, C) æ ¼å¼ï¼Œç›´æ¥å½’ä¸€åŒ–å³å¯
                return tensor_img.float() / 255.0

            except Exception as e:
                # ... (é”™è¯¯å¤„ç†) ...
                # âŒ [åˆ é™¤] return torch.zeros((3, 224, 224), dtype=torch.float32)
                # âœ… [ä¿®æ”¹] ä¿æŒä¸€è‡´çš„ (H, W, C) æ ¼å¼
                print(f"âŒ [DEBUG] Error loading video {video_path}: {e}")
                return torch.zeros((224, 224, 3), dtype=torch.float32)

    def __getitem__(self, idx):


        # 1. åŸºç¡€ä¿¡æ¯
        ep_idx, idx_in_seg = self._get_info_by_idx(idx)
        
        rel_path = self.video_paths[ep_idx] # "327/648642"
        
        start_frame, end_frame = self.start_end[ep_idx]
        current_abs_frame = start_frame + idx_in_seg

        # 2. è·å–è¯¥ Episode çš„ Metadata
        if rel_path in self.dims_meta:
            meta_info = self.dims_meta[rel_path]
            dim_list = meta_info["dim_list"]
            total_file_length = meta_info["length"] # è¿™ä¸€é›†çš„æ€»å¸§æ•° (action.npy çš„è¡Œæ•°)
        else:
            raise ValueError(f"Meta info not found for {rel_path}")

        # ==========================================
        # 3. è¯»å–è§†é¢‘
        # ==========================================
        video_folder = os.path.join(self.root_dir, "videos_h264", rel_path, "videos")

        img_head = self._load_video_frame(video_folder, "head_color", current_abs_frame)
        img_left = self._load_video_frame(video_folder, "hand_left_color", current_abs_frame)
        img_right = self._load_video_frame(video_folder, "hand_right_color", current_abs_frame)

        # ==========================================
        # 4. è¯»å– Action & State (ä½¿ç”¨ Helper ç±»)
        # ==========================================
        action_path = os.path.join(self.root_dir, "actions_gaussian", rel_path, "action.npy")

        # print("Action Path:", action_path)
        
        # è®¡ç®—è¯»å–èŒƒå›´
        # æˆ‘ä»¬è¦è¯» [current, current + horizon]
        # ä½†å¿…é¡»é™åˆ¶åœ¨ total_file_length ä»¥å†…ï¼Œå¦åˆ™ load_range é‡Œçš„ reshape ä¼šæŠ¥é”™
        read_start = current_abs_frame
        read_end = min(current_abs_frame + self.action_horizon, total_file_length)
        
        # [ä¿®æ”¹ 2] ä½¿ç”¨ AgibotActionState è¯»å–
        # è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ seek + readï¼Œéå¸¸å¿«
        try:
            action_obj = AgibotActionState.load_range_from_path(
                path=action_path,
                dim_list=dim_list,
                start=read_start,
                end=read_end
            )
        except Exception as e:
            # å®¹é”™ï¼šå¦‚æœè¯»å–å¤±è´¥ï¼Œè¿”å›å…¨0
            logging.error(f"Error reading {action_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„ action_obj ç»“æ„
            action_obj = AgibotActionState()
            # å¡«å……å…¨0æ•°æ® (L, Dim)
            fake_len = read_end - read_start
            action_obj.state_joint_position = np.zeros((fake_len, 14), dtype=np.float32)
            action_obj.action_effector_position = np.zeros((fake_len, 2), dtype=np.float32)

        # ==========================================
        # 5. ç»„è£…æ•°æ® (State & Action)
        # ==========================================
        
        # A. å½“å‰ State (Abs)
        # State = State_joint_position (14) + Action_effector_position (2)
        # å– index 0 (å³ current_abs_frame)
        curr_joint = action_obj.state_joint_position[0]
        curr_gripper = action_obj.action_effector_position[0]
        
        state_t_16 = np.concatenate([curr_joint, curr_gripper]) # (16,)

        # B. æœªæ¥ Actions (Delta/Abs)
        # Future Joint (14) & Future Gripper (2)
        future_joints = action_obj.state_joint_position
        future_gripper = action_obj.action_effector_position
        
        # Delta Joint = Future - Current
        joint_delta = future_joints - curr_joint
        
        # Action = [Joint Delta (14), Gripper Abs (2)]
        actions_16 = np.concatenate([joint_delta, future_gripper], axis=1) # (L, 16)
        valid_len = actions_16.shape[0]

        # ==========================================
        # 6. Padding & Output
        # ==========================================
        
        # State Padding
        state_final = torch.zeros(32, dtype=torch.float32)
        state_final[:16] = torch.from_numpy(state_t_16)
        
        # Action Padding
        actions_final = torch.zeros((self.action_horizon, 32), dtype=torch.float32)
        actions_segment = torch.from_numpy(actions_16)
        
        actions_final[:valid_len, :16] = actions_segment
        
        # Repeat Padding (è¡¥é½ä¸è¶³ Horizon çš„éƒ¨åˆ†)
        if valid_len < self.action_horizon:
            actions_final[valid_len:, :16] = actions_segment[-1]
            
        action_is_pad = torch.zeros(self.action_horizon, dtype=torch.bool)
        action_is_pad[valid_len:] = True

        # ==========================================
        # 7. æå– State Head & Waist (ä»…å½“å‰å¸§)
        # ==========================================
        state_head = torch.from_numpy(action_obj.state_head_position[0])   # (2,)
        state_waist = torch.from_numpy(action_obj.state_waist_position[0]) # (2,)

        # ==========================================================
        # 8. Normalization using q01 / q99 (MODIFIED)
        # Formula: 2 * (x - q01) / (q99 - q01) - 1
        # ==========================================================

        if self.normalization:
            # åªå¯¹å…³èŠ‚ 14 ç»´åš q01/q99 å½’ä¸€åŒ–
            VALID_DIMS_JOINT = self.JOINT_DIM  # = 14

            # ---- STATE: åªå½’ä¸€åŒ– state_final[:14] ----
            state_q01 = self.state_q01[:VALID_DIMS_JOINT]
            state_q99 = self.state_q99[:VALID_DIMS_JOINT]

            state_denom = state_q99 - state_q01
            state_denom = torch.where(
                torch.abs(state_denom) < 1e-3,
                torch.full_like(state_denom, 1e-3),
                state_denom,
            )

            state_joint = state_final[:VALID_DIMS_JOINT]
            state_joint = 2 * (state_joint - state_q01) / state_denom - 1
            state_joint = torch.clamp(state_joint, -10.0, 10.0)
            state_final[:VALID_DIMS_JOINT] = state_joint
            # state_final[14:16] æ˜¯ gripperï¼Œä¿æŒåŸå€¼ï¼ˆæˆ–ä½ åé¢æƒ³å•ç‹¬å¤„ç†ï¼‰
            # state_final[16:] æ˜¯ paddingï¼Œä¸å½’ä¸€åŒ–

            # ---- ACTIONS: åªå½’ä¸€åŒ– actions_final[:, :14] ----
            action_q01 = self.action_q01[:VALID_DIMS_JOINT]
            action_q99 = self.action_q99[:VALID_DIMS_JOINT]

            action_denom = action_q99 - action_q01
            action_denom = torch.where(
                torch.abs(action_denom) < 1e-3,
                torch.full_like(action_denom, 1e-3),
                action_denom,
            )

            act_joint = actions_final[:, :VALID_DIMS_JOINT]
            act_joint = 2 * (act_joint - action_q01) / action_denom - 1
            act_joint = torch.clamp(act_joint, -10.0, 10.0)
            actions_final[:, :VALID_DIMS_JOINT] = act_joint


        return {
            "head": img_head,
            "left_gripper": img_left,
            "right_gripper": img_right,
            "head_mask": torch.tensor(True),
            "left_gripper_mask": torch.tensor(True),
            "right_gripper_mask": torch.tensor(True),
            "states": state_final,
            "actions": actions_final,

            # Auxiliary Data (Current Frame Only)
            "state_head": state_head,     # (2,)
            "state_waist": state_waist,   # (2,)

            # --- åœ¨è¿™é‡Œæ·»åŠ  rel_path ---
            "rel_path": rel_path,

            "prompt": self.instructions[ep_idx],
            # Meta
            "episode_index": torch.tensor(ep_idx),
            "frame_index": torch.tensor(current_abs_frame),
            "timestamp": torch.tensor(current_abs_frame / 30.0),
            "next.done": torch.tensor(current_abs_frame == end_frame - 1),
            "action_is_pad": action_is_pad
        }



                    # actions_final[:, 14:16] æ˜¯ gripperï¼Œä¿æŒåŸå€¼
            # actions_final[:, 16:] æ˜¯ paddingï¼Œä¸å½’ä¸€åŒ–


        # # æ„é€ æœ€ç»ˆè¿”å›çš„å­—å…¸
        # result = {
        #     "head": img_head,
        #     "left_gripper": img_left,
        #     "right_gripper": img_right,
        #     "states": state_final,
        #     "actions": actions_final,
        #     "prompt": self.instructions[ep_idx],
        #     # Meta
        #     "episode_index": torch.tensor(ep_idx),
        #     "frame_index": torch.tensor(current_abs_frame),
        #     "timestamp": torch.tensor(current_abs_frame / 30.0),
        #     "next.done": torch.tensor(current_abs_frame == end_frame - 1),
        #     "action_is_pad": action_is_pad
        # }

        # print("\n" + "="*60)
        # print(f"ğŸ” [Dataset Debug] Index: {idx} | RelPath: {rel_path}")
        
        # # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ [æ–°å¢] æ‰“å° Action/State è¯»å–è·¯å¾„ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # print(f"   ğŸ“‚ Action/State File Path: {action_path}") 
        # # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

        # # --- 1. æ‰“å° Prompt (æœ€å…³é”®) ---
        # print(f"   ğŸ“ Prompt: \"{result['prompt']}\"")

        # # --- 2. æ‰“å° Image é‡‡æ · (æ£€æŸ¥æ˜¯å¦å…¨é»‘) ---
        # print("   ğŸ–¼ï¸  Image Check (Center Pixel [112,112] RGB):")
        # for k in ["head", "left_gripper", "right_gripper"]:
        #     if k in result:
        #         img = result[k]
        #         # å–ä¸­é—´ä¸€ä¸ªåƒç´ çš„å€¼ï¼Œä¿ç•™4ä½å°æ•°
        #         center_px = img[:, 112, 112].tolist() 
        #         formatted_px = [round(x, 4) for x in center_px]
        #         print(f"      |-- {k:<15}: {formatted_px} (Min: {img.min():.4f}, Max: {img.max():.4f})")

        # # --- 3. æ‰“å° State/Action å…·ä½“æ•°å€¼ (å®Œæ•´ç‰ˆ) ---
        # print("   ğŸ”¢ FULL Vector Values:")
        
        # # (A) æ‰“å°å®Œæ•´çš„ State (32ç»´)
        # states_list = result['states'].tolist()
        # # æ ¼å¼åŒ–ä¸€ä¸‹ï¼Œä¿ç•™4ä½å°æ•°ï¼Œæ–¹ä¾¿é˜…è¯»
        # states_str = ", ".join([f"{x: .4f}" for x in states_list])
        # print(f"      |-- states (32):")
        # print(f"          [{states_str}]")
        
        # # (B) æ‰“å°å®Œæ•´çš„ Actions (30 x 32)
        # # æˆ‘ä»¬é€å¸§æ‰“å°ï¼Œè¿™æ ·ä½ çœ‹å¾—æ¸…æ¥šæ¯ä¸€å¸§çš„å˜åŒ–
        # actions_np = result['actions'].numpy()
        # print(f"      |-- actions ({actions_np.shape}):")
        # for t in range(actions_np.shape[0]):
        #     # è·å–å½“å‰æ—¶é—´æ­¥çš„ 32 ç»´å‘é‡
        #     act_row = actions_np[t]
        #     # æ ¼å¼åŒ–å­—ç¬¦ä¸²
        #     row_str = ", ".join([f"{x: .4f}" for x in act_row])
            
        #     # æ£€æŸ¥è¿™ä¸€è¡Œæ˜¯ä¸æ˜¯å…¨ 0 (è¾…åŠ©åˆ¤æ–­)
        #     is_zero = np.allclose(act_row, 0, atol=1e-5)
        #     zero_tag = "âš ï¸ ALL ZERO" if is_zero else ""
            
        #     print(f"          [Step {t:02d}]: {row_str} {zero_tag}")
        
        # print("="*60 + "\n")

        # while True:
        #     pass