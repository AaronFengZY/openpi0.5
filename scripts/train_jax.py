import dataclasses
import functools
import logging
import platform
from typing import Any

import etils.epath as epath
import flax.nnx as nnx
from flax.training import common_utils
import flax.traverse_util as traverse_util
import jax
import jax.experimental
import jax.numpy as jnp
import numpy as np
import optax
import tqdm_loggable.auto as tqdm
import wandb


import os
from PIL import Image
import time

import openpi.models.model as _model
import openpi.shared.array_typing as at
import openpi.shared.nnx_utils as nnx_utils
import openpi.training.checkpoints as _checkpoints
import openpi.training.config as _config
import openpi.training.data_loader as _data_loader
import openpi.training.optimizer as _optimizer
import openpi.training.sharding as sharding
import openpi.training.utils as training_utils
import openpi.training.weight_loaders as _weight_loaders

import json
import pathlib


def init_logging():
    """Custom logging format for better readability."""
    level_mapping = {"DEBUG": "D", "INFO": "I", "WARNING": "W", "ERROR": "E", "CRITICAL": "C"}

    class CustomFormatter(logging.Formatter):
        def format(self, record):
            record.levelname = level_mapping.get(record.levelname, record.levelname)
            return super().format(record)

    formatter = CustomFormatter(
        fmt="%(asctime)s.%(msecs)03d [%(levelname)s] %(message)-80s (%(process)d:%(filename)s:%(lineno)s)",
        datefmt="%H:%M:%S",
    )

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers[0].setFormatter(formatter)


def init_wandb(config: _config.TrainConfig, *, resuming: bool, log_code: bool = False, enabled: bool = True):
    if not enabled:
        wandb.init(mode="disabled")
        return

    ckpt_dir = config.checkpoint_dir
    # Rank 0 è´Ÿè´£åˆ›å»ºç›®å½•ï¼Œå…¶ä»–èŠ‚ç‚¹ç­‰å¾… (é€šå¸¸ initialize_checkpoint_dir å·²ç»åšäº†ï¼Œè¿™é‡Œåªæ˜¯ä¿é™©)
    
    run_id = None
    group_name = config.exp_name  # ä½¿ç”¨å®éªŒåä½œä¸º Group

    # 1. ç¡®å®š Run ID
    if resuming:
        try:
            # å°è¯•è¯»å– Run ID
            run_id = (ckpt_dir / "wandb_id.txt").read_text().strip()
        except Exception as e:
            logging.warning(f"âš ï¸ [Rank {jax.process_index()}] Failed to read wandb_id: {e}")
            # å¦‚æœè¯»ä¸åˆ°ï¼Œå°±å½“æ–°è®­ç»ƒå¤„ç†? æˆ–è€…æŠ›å‡ºå¼‚å¸¸? è§†éœ€æ±‚è€Œå®šã€‚
            # è¿™é‡Œå¦‚æœä¸æŠ›å¼‚å¸¸ï¼Œåé¢ wandb.init(id=None) ä¼šåˆ›å»ºæ–°çš„ runï¼Œè¿™å¯èƒ½ä¸æ˜¯ä½ æƒ³è¦çš„ã€‚
            # ä½†å¯¹äº Rank > 0ï¼Œå¦‚æœè¯»ä¸åˆ°ï¼Œå…¶å®é—®é¢˜ä¸å¤§ï¼Œå› ä¸ºå®ƒä»¬åªæ˜¯é™„å±è¿›ç¨‹ã€‚
            pass
    
    # 2. ç¡®å®š Run Name (å¦‚æœæ˜¯æ–°è®­ç»ƒ)
    if not run_id:
        # å¦‚æœæ²¡æœ‰ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘
        env_run_name = os.getenv("WANDB_RUN_NAME")
        if env_run_name:
            run_name = env_run_name
        else:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            run_name = f"{config.exp_name}_{timestamp}"
    else:
        run_name = None # Resume æ¨¡å¼ä¸‹ name é€šå¸¸è¢«å¿½ç•¥æˆ–ç”± server å†³å®š

    # 3. ç»Ÿä¸€åˆå§‹åŒ–
    # æ¯ä¸ªèŠ‚ç‚¹éƒ½æ‰§è¡Œï¼Œç¡®ä¿éƒ½èƒ½ä¸Šä¼  System Metrics
    wandb.init(
        id=run_id,
        name=run_name,
        resume="must" if run_id else "allow",
        config=dataclasses.asdict(config),
        project=config.project_name,
        group=group_name, # å…³é”®ï¼šè®©å®ƒä»¬åœ¨ç½‘é¡µä¸Šèšåœ¨ä¸€èµ·
    )

    # 4. [Rank 0 ç‹¬å ] ä¿å­˜ ID (ä»…å½“æ˜¯æ–°è®­ç»ƒæ—¶)
    if not resuming and jax.process_index() == 0:
        try:
            (ckpt_dir / "wandb_id.txt").write_text(wandb.run.id)
            logging.info(f"âœ… [Rank 0] Saved wandb_id to {ckpt_dir}")
        except Exception as e:
            logging.warning(f"âš ï¸ [Rank 0] Failed to save wandb_id: {e}")

    # 5. [Rank 0 ç‹¬å ] Log Code
    if log_code and jax.process_index() == 0:
        wandb.run.log_code(epath.Path(__file__).parent.parent)

def _load_weights_and_validate(loader: _weight_loaders.WeightLoader, params_shape: at.Params) -> at.Params:
    """Loads and validates the weights. Returns a loaded subset of the weights."""
    loaded_params = loader.load(params_shape)
    at.check_pytree_equality(expected=params_shape, got=loaded_params, check_shapes=True, check_dtypes=True)

    # Remove jax.ShapeDtypeStruct from the loaded params. This makes sure that only the loaded params are returned.
    return traverse_util.unflatten_dict(
        {k: v for k, v in traverse_util.flatten_dict(loaded_params).items() if not isinstance(v, jax.ShapeDtypeStruct)}
    )


@at.typecheck
def init_train_state(
    config: _config.TrainConfig, init_rng: at.KeyArrayLike, mesh: jax.sharding.Mesh, *, resume: bool
) -> tuple[training_utils.TrainState, Any]:
    tx = _optimizer.create_optimizer(config.optimizer, config.lr_schedule, weight_decay_mask=None)

    def init(rng: at.KeyArrayLike, partial_params: at.Params | None = None) -> training_utils.TrainState:
        rng, model_rng = jax.random.split(rng)
        # initialize the model (and its parameters).
        model = config.model.create(model_rng)

        # Merge the partial params into the model.
        if partial_params is not None:
            graphdef, state = nnx.split(model)
            # This will produce an error if the partial params are not a subset of the state.
            state.replace_by_pure_dict(partial_params)
            model = nnx.merge(graphdef, state)

        params = nnx.state(model)
        # Convert frozen params to bfloat16.
        params = nnx_utils.state_map(params, config.freeze_filter, lambda p: p.replace(p.value.astype(jnp.bfloat16)))

        return training_utils.TrainState(
            step=0,
            params=params,
            model_def=nnx.graphdef(model),
            tx=tx,
            opt_state=tx.init(params.filter(config.trainable_filter)),
            ema_decay=config.ema_decay,
            ema_params=None if config.ema_decay is None else params,
        )

    train_state_shape = jax.eval_shape(init, init_rng)
    state_sharding = sharding.fsdp_sharding(train_state_shape, mesh, log=True)

    if resume:
        return train_state_shape, state_sharding

    partial_params = _load_weights_and_validate(config.weight_loader, train_state_shape.params.to_pure_dict())
    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())

    # Initialize the train state and mix in the partial params.
    train_state = jax.jit(
        init,
        donate_argnums=(1,),  # donate the partial params buffer.
        in_shardings=replicated_sharding,
        out_shardings=state_sharding,
    )(init_rng, partial_params)

    return train_state, state_sharding


@at.typecheck
def train_step(
    config: _config.TrainConfig,
    rng: at.KeyArrayLike,
    state: training_utils.TrainState,
    batch: tuple[_model.Observation, _model.Actions],
) -> tuple[training_utils.TrainState, dict[str, at.Array]]:
    model = nnx.merge(state.model_def, state.params)
    model.train()

    @at.typecheck
    def loss_fn(
        model: _model.BaseModel, rng: at.KeyArrayLike, observation: _model.Observation, actions: _model.Actions
    ):
        chunked_loss = model.compute_loss(rng, observation, actions, train=True)
        return jnp.mean(chunked_loss)

    train_rng = jax.random.fold_in(rng, state.step)
    observation, actions = batch

    # Filter out frozen params.
    diff_state = nnx.DiffState(0, config.trainable_filter)
    loss, grads = nnx.value_and_grad(loss_fn, argnums=diff_state)(model, train_rng, observation, actions)

    # grads = jax.tree.map(lambda g: jax.lax.pmean(g, axis_name="dp"), grads)
    # loss = jax.lax.pmean(loss, axis_name="dp")  # å¯é€‰ï¼Œä½†æˆ‘å»ºè®®åŠ ä¸Šï¼Œæ—¥å¿—æ›´ä¸€è‡´

    params = state.params.filter(config.trainable_filter)
    updates, new_opt_state = state.tx.update(grads, state.opt_state, params)
    new_params = optax.apply_updates(params, updates)

    # Update the model in place and return the new full state.
    nnx.update(model, new_params)
    new_params = nnx.state(model)

    new_state = dataclasses.replace(state, step=state.step + 1, params=new_params, opt_state=new_opt_state)
    if state.ema_decay is not None:
        new_state = dataclasses.replace(
            new_state,
            ema_params=jax.tree.map(
                lambda old, new: state.ema_decay * old + (1 - state.ema_decay) * new, state.ema_params, new_params
            ),
        )

    # Filter out params that aren't kernels.
    kernel_params = nnx.state(
        model,
        nnx.All(
            nnx.Param,
            nnx.Not(nnx_utils.PathRegex(".*/(bias|scale|pos_embedding|input_embedding)")),
            lambda _, x: x.value.ndim > 1,
        ),
    )
    info = {
        "loss": loss,
        "grad_norm": optax.global_norm(grads),
        "param_norm": optax.global_norm(kernel_params),
    }
    return new_state, info


def main(config: _config.TrainConfig):
    init_logging()

    # =================================================================
    # [å…¼å®¹æ€§ä¿®æ”¹] JAX åˆ†å¸ƒå¼åˆå§‹åŒ– (è‡ªåŠ¨é€‚é… Single/Multi Node)
    # =================================================================
    # é€»è¾‘ï¼šåªæœ‰å½“ç¯å¢ƒå˜é‡ä¸­å­˜åœ¨ RANK (ç”± AMLT/Torchrun æ³¨å…¥) æ—¶æ‰åˆå§‹åŒ–åˆ†å¸ƒå¼
    # å¦åˆ™é»˜è®¤å•æœºè¿è¡Œï¼Œä¸æŠ¥é”™ã€‚
    
    os.environ["JAX_COORDINATION_SERVICE_TIMEOUT_SEC"] = "3600"
    logging.info(f"â° [Python] Set JAX timeout to 3600s to survive slow startup.")

    if os.environ.get('RANK'):
        try:
            # 1. è·å–åˆ†å¸ƒå¼å‚æ•°
            rank = int(os.environ['RANK'])
            world_size = int(os.environ['WORLD_SIZE'])
            master_addr = os.environ.get('MASTER_ADDR', 'localhost')
            master_port = os.environ.get('MASTER_PORT', '12355')
            coordinator_address = f"{master_addr}:{master_port}"

            logging.info(f"ğŸŒ [Dist] Mode Detected. Rank: {rank}/{world_size}, Master: {coordinator_address}")

            # 2. åˆå§‹åŒ– JAX åˆ†å¸ƒå¼
            jax.distributed.initialize(
                coordinator_address=coordinator_address,
                num_processes=world_size,
                process_id=rank,
                local_device_ids=None, # è®© JAX è‡ªåŠ¨æ£€æµ‹æœ¬åœ°å¯è§çš„ GPU
                initialization_timeout=3600  # è®¾ç½®ä¸º 1 å°æ—¶
            )
            logging.info(f"âœ… [Dist] Initialized! Global Device Count: {jax.device_count()}")
            
        except Exception as e:
            logging.error(f"âŒ [Dist] Initialization failed: {e}")
            raise e
    else:
        logging.info("â„¹ï¸ [Dist] No RANK found. Running in SINGLE-NODE mode.")

    logging.info(f"Running on node: {platform.node()}")


    # logging.info(f"Running on: {platform.node()}")

    if config.batch_size % jax.device_count() != 0:
        raise ValueError(
            f"Batch size {config.batch_size} must be divisible by the number of devices {jax.device_count()}."
        )

    jax.config.update("jax_compilation_cache_dir", str(epath.Path("~/.cache/jax").expanduser()))

    rng = jax.random.key(config.seed)
    train_rng, init_rng = jax.random.split(rng)

    # --- build 2D mesh: dp (nodes) x fsdp (local gpus) ---
    num_nodes = jax.process_count()              # 4
    local_gpus = jax.local_device_count()        # 8
    assert jax.device_count() == num_nodes * local_gpus, (jax.device_count(), num_nodes, local_gpus)

    devices_2d = np.array(jax.devices()).reshape((num_nodes, local_gpus))
    mesh = jax.sharding.Mesh(devices_2d, axis_names=("dp", "fsdp"))

    # shard batch over BOTH dp and fsdp so global batch is distributed across 32 devices
    data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(("dp", "fsdp"),))

    # replicated scalar / rng etc
    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())

    # =================================================================
    # [å…³é”®ä¿®å¤] åˆ†å¸ƒå¼å®‰å…¨æ¸…ç†ï¼šåªæœ‰ Rank 0 è´Ÿè´£ Overwrite åˆ é™¤
    # =================================================================
    # ç›®çš„ï¼šé˜²æ­¢å¤šä¸ªèŠ‚ç‚¹åŒæ—¶æ‰§è¡Œ rmtree å¯¼è‡´ FileNotFoundError
    if config.overwrite and jax.process_index() == 0:
        ckpt_path = epath.Path(config.checkpoint_dir)
        if ckpt_path.exists():
            logging.info(f"ğŸ§¹ [Rank 0] Overwrite flag is set. Cleaning up {ckpt_path}...")
            import shutil
            try:
                # ä½¿ç”¨ shutil å¼ºåŠ›åˆ é™¤ï¼Œä¸ç”¨ etils é˜²æ­¢ backend å…¼å®¹é—®é¢˜
                if ckpt_path.is_dir():
                    shutil.rmtree(str(ckpt_path))
                else:
                    ckpt_path.unlink()
                logging.info("âœ… [Rank 0] Cleanup done.")
            except Exception as e:
                logging.warning(f"âš ï¸ [Rank 0] Cleanup failed (might be deleted already): {e}")

    # å¿…é¡»è®©å…¶ä»–èŠ‚ç‚¹ç­‰å¾… Rank 0 åˆ å®Œï¼Œå¦åˆ™å®ƒä»¬å¯èƒ½ä¼šè¯•å›¾åˆ›å»ºä¸€ä¸ªæ­£åœ¨è¢«åˆ é™¤çš„ç›®å½•
    if config.overwrite:
        # ä½¿ç”¨ JAX çš„ barrier (æˆ–è€…ç®€å•çš„ sleep)
        logging.info(f"â³ [Rank {jax.process_index()}] Waiting for Rank 0 to cleanup...")
        # å¦‚æœæ²¡æœ‰å¾ˆå¥½çš„ barrier æœºåˆ¶ï¼Œç®€å•çš„ sleep ä¹Ÿèƒ½è§£å†³å¤§éƒ¨åˆ†é—®é¢˜ï¼Œæˆ–è€…ä¾èµ–åé¢çš„ initialize_checkpoint_dir è‡ªåŠ¨é‡å»º
        time.sleep(5) 

    # =================================================================

    checkpoint_manager, resuming = _checkpoints.initialize_checkpoint_dir(
        config.checkpoint_dir,
        keep_period=config.keep_period,
        overwrite=config.overwrite,
        resume=config.resume,
    )
    init_wandb(config, resuming=resuming, enabled=config.wandb_enabled)

    # =================================================================
    # ğŸš¨ [Auto-Config v3] ç»ˆæè¡¥ä¸ï¼šè¦†ç›– Stats + ç§»é™¤ ResizeImages
    # =================================================================
    
    # 1. å‡†å¤‡ Stats æ•°æ®
    env_stats_path = os.getenv("NORM_STATS_FILE")
    real_stats = None
    
    if env_stats_path:
        logging.info(f"ğŸ”„ [Auto-Config] Loading stats from: {env_stats_path}")
        with open(env_stats_path, 'r') as f:
            loaded = json.load(f)
            real_stats = loaded.get("norm_stats", loaded)
    else:
        # å¦‚æœæ²¡ç¯å¢ƒå˜é‡ï¼Œé€ ä¸ªå‡çš„é˜²æ­¢æŠ¥é”™ (Datasetå†…éƒ¨å·²å¤„ç†å½’ä¸€åŒ–)
        logging.info(f"âš ï¸ [Auto-Config] No Env Stats, using dummy stats.")
        dummy = {"mean": np.zeros(32), "std": np.ones(32), "q01": np.zeros(32), "q99": np.ones(32)}
        real_stats = {"state": dummy, "actions": dummy}

# =================================================================
    # ğŸš¨ [Auto-Config v4] ç»ˆæè¡¥ä¸ï¼šStats + NoResize + Repackä¿®æ­£
    # =================================================================
    
    # 1. å‡†å¤‡ Stats
    env_stats_path = os.getenv("NORM_STATS_FILE")
    real_stats = None
    
    if env_stats_path:
        logging.info(f"ğŸ”„ [Auto-Config] Loading stats from: {env_stats_path}")
        with open(env_stats_path, 'r') as f:
            loaded = json.load(f)
            real_stats = loaded.get("norm_stats", loaded)
    else:
        logging.info(f"âš ï¸ [Auto-Config] No Env Stats, using dummy stats.")
        dummy = {"mean": np.zeros(32), "std": np.ones(32), "q01": np.zeros(32), "q99": np.ones(32)}
        real_stats = {"state": dummy, "actions": dummy}

    # 2. å®šä¹‰è¡¥ä¸å‡½æ•°
    original_create = config.data.create
    import openpi.transforms as _transforms 

    def patched_create(assets_dirs, model_config):
        # A. åŸå§‹é€»è¾‘
        data_cfg = original_create(assets_dirs, model_config)
        
        # B. ã€æ¨¡å‹å˜æ¢ã€‘åªä¿ç•™ Tokenize å’Œ Paddingï¼Œå‰”é™¤ Resize
        existing_inputs = data_cfg.model_transforms.inputs
        filtered_inputs = []
        for t in existing_inputs:
            t_name = t.__class__.__name__
            if "ResizeImages" in t_name or "PadStatesAndActions" in t_name:
                logging.info(f"âœ‚ï¸ [Patch] Removing Model Transform: {t_name}")
                continue
            filtered_inputs.append(t)
        new_model_transforms = _transforms.Group(inputs=filtered_inputs, outputs=[])

        # C. ã€æ•°æ®å˜æ¢ã€‘ç½®ç©º (ç§»é™¤ Libero)
        empty_data_transforms = _transforms.Group(inputs=[], outputs=[])

        # D. ã€å…³é”®æ–°å¢ã€‘ä¿®æ­£ RepackTransform (æ¬å®¶)
        # ç›®çš„ï¼šå»æ‰ 'observation/' å‰ç¼€ï¼Œæ»¡è¶³ Model.from_dict çš„éœ€æ±‚
        # åŒæ—¶æ„é€  image å­—å…¸ç»“æ„
        new_repack_transforms = _transforms.Group(
            inputs=[_transforms.RepackTransform({
                # 1. å›¾åƒ (ä¿æŒä¸å˜)
                "image/base_0_rgb": "head",              
                "image/left_wrist_0_rgb": "left_gripper",
                "image/right_wrist_0_rgb": "right_gripper",
                
                # 2. [æ–°å¢] å›¾åƒ Mask (å¿…é¡»ä¸€ä¸€å¯¹åº”)
                # æ¨¡å‹ä¼šåœ¨ data["image_mask"] ä¸‹å¯»æ‰¾å¯¹åº”çš„é”®
                "image_mask/base_0_rgb": "head_mask",
                "image_mask/left_wrist_0_rgb": "left_gripper_mask",
                "image_mask/right_wrist_0_rgb": "right_gripper_mask",
                
                # 3. çŠ¶æ€ (ä¿æŒä¸å˜)
                "state": "states",                       
                "actions": "actions",
                "prompt": "prompt",
            })]
        )

        # E. ç»„è£…æ‰€æœ‰ä¿®æ”¹
        logging.info("ğŸ’‰ [Patch] Applying FULL Fixes (Stats, Resize, Repack)...")
        
        data_cfg = dataclasses.replace(
            data_cfg, 
            norm_stats=real_stats,          
            use_quantile_norm=False,        
            asset_id="agibot_full",         
            model_transforms=new_model_transforms, 
            data_transforms=empty_data_transforms,
            repack_transforms=new_repack_transforms # <--- æ³¨å…¥æ–°çš„ Repack
        )
        return data_cfg
    
    # 3. æŒ‚è½½è¡¥ä¸
    object.__setattr__(config.data, "create", patched_create)
    
    logging.info("âœ… [Auto-Config] Patch applied successfully!")

    # =================================================================
    # 3. [å…³é”®ä¿®æ”¹] ä¿®æ”¹ Seed é˜²æ­¢å¤šæœºæ•°æ®é‡å¤
    # =================================================================
    # å¿…é¡»åœ¨ create_data_loader ä¹‹å‰æ‰§è¡Œ
    current_process_id = jax.process_index() 
    if current_process_id > 0:
        logging.info(f"ğŸ² [Dist] Shifting data seed for Rank {current_process_id} to avoid duplication.")
        
        # ã€ä¿®æ­£ã€‘ä½¿ç”¨ replace åˆ›å»ºä¸€ä¸ªæ–°çš„ config å¯¹è±¡
        config = dataclasses.replace(config, seed=config.seed + current_process_id)
        
    logging.info(f"ğŸ“‰ Creating Data Loader with Seed: {config.seed}")

    data_loader = _data_loader.create_data_loader(
        config,
        sharding=data_sharding,
        shuffle=True,
    )
    data_iter = iter(data_loader)
    batch = next(data_iter)
    logging.info(f"Initialized data loader:\n{training_utils.array_tree_to_info(batch)}")


    # --------------------------------------------------------------------------
    # [å¯è§†åŒ–ä¿®å¤] æ‰“å°è°ƒè¯•ä¿¡æ¯å¹¶ä¿®å¤ WandB å›¾ç‰‡æ ¼å¼ (Channel First -> Last)
    # --------------------------------------------------------------------------
    def get_local_numpy(jax_array):
        # å¦‚æœæ˜¯æ™®é€š numpy æ•°ç»„ç›´æ¥è¿”å›
        if isinstance(jax_array, (np.ndarray, jnp.ndarray)) and not hasattr(jax_array, 'addressable_shards'):
            return np.array(jax_array)
            
        try:
            # è·å–æœ¬åœ°å¯è§çš„æ‰€æœ‰åˆ†ç‰‡ (Addressable Shards)
            # æ¯ä¸ª shard.data æ˜¯å­˜å‚¨åœ¨æœ¬åœ° GPU ä¸Šçš„ jax.Array
            local_shards = jax_array.addressable_shards
            if not local_shards:
                return np.array([]) # ç†è®ºä¸Šä¸åº”å‘ç”Ÿ
            
            # å°†æ‰€æœ‰æœ¬åœ°åˆ†ç‰‡æ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆå½“å‰èŠ‚ç‚¹çš„ Local Batch
            # ä¾‹å¦‚: å…¨å±€ 512ï¼Œæœ¬åœ°å°±æ˜¯ 256
            local_data = np.concatenate([np.array(s.data) for s in local_shards], axis=0)
            return local_data
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to gather local shards: {e}")
            return np.array([])

    obs, act = batch
    logging.info("=== DEBUG: Checking Batch Data Structure (Local Only) ===")
    # 1. æ‰“å°åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ (ä½¿ç”¨ get_local_numpy)
    for k in ["base_0_rgb", "left_wrist_0_rgb", "right_wrist_0_rgb"]:
        if k in obs.images:
            # ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨ get_local_numpy æ›¿ä»£ np.array()
            img_tensor = get_local_numpy(obs.images[k])
            mask_tensor = get_local_numpy(obs.image_masks[k])
            
            if img_tensor.size > 0:
                logging.info(f"[{k}] LocalShape: {img_tensor.shape}, Mean: {img_tensor.mean():.4f}, Valid: {mask_tensor.sum()}/{mask_tensor.size}")

    # 2. ç”Ÿæˆ WandB å¯è§†åŒ–å›¾ç‰‡
    # åªåœ¨ Rank 0 ä¸Šåšï¼Œä¸”åªç”»æœ¬åœ°çš„å›¾ç‰‡
    if jax.process_index() == 0: 
        try:
            images_to_log = []
            first_key = next(iter(obs.images.keys()), None)
            
            if first_key:
                # è·å–è¯¥ Key çš„æœ¬åœ°æ•°æ®æ¥ç¡®å®šé•¿åº¦
                local_sample_img = get_local_numpy(obs.images[first_key])
                batch_size_vis = len(local_sample_img)
                
                # æœ€å¤šæ˜¾ç¤º 5 å¼ 
                for i in range(min(5, batch_size_vis)):
                    imgs_list = []
                    for img_key, img_jax_array in obs.images.items():
                        # ã€å…³é”®ä¿®æ”¹ã€‘è·å–æœ¬åœ°æ•°æ®
                        local_batch = get_local_numpy(img_jax_array)
                        if len(local_batch) <= i: continue

                        arr = local_batch[i] # å–å‡ºç¬¬ i å¼ å›¾
                        
                        # å½’ä¸€åŒ–å¤„ç†
                        if arr.min() < 0:
                            arr = ((arr + 1.0) / 2.0 * 255).astype(np.uint8)
                        elif arr.max() <= 1.0:
                            arr = (arr * 255).astype(np.uint8)
                        else:
                            arr = arr.astype(np.uint8)

                        imgs_list.append(arr)
                    
                    if imgs_list:
                        concat_img = np.concatenate(imgs_list, axis=1)
                        images_to_log.append(wandb.Image(concat_img, caption=f"Sample {i}"))

                if images_to_log:
                    wandb.log({"camera_views": images_to_log}, step=0)
                    logging.info("âœ… [Viz] Camera views logged to WandB successfully.")
            else:
                logging.warning("âš ï¸ [Viz] No images found in batch to log.")
                
        except Exception as e:
            logging.warning(f"âš ï¸ [Viz] Failed to log images to WandB: {e}")
            # æ‰“å°è¯¦ç»†æŠ¥é”™æ–¹ä¾¿è°ƒè¯•ï¼Œä½†ä¸ä¸­æ–­è®­ç»ƒ
            import traceback
            traceback.print_exc()

    # # Log images from first batch to sanity check.
    # images_to_log = [
    #     wandb.Image(np.concatenate([np.array(img[i]) for img in batch[0].images.values()], axis=1))
    #     for i in range(min(5, len(next(iter(batch[0].images.values())))))
    # ]
    # wandb.log({"camera_views": images_to_log}, step=0)

    train_state, train_state_sharding = init_train_state(config, init_rng, mesh, resume=resuming)
    jax.block_until_ready(train_state)
    logging.info(f"Initialized train state:\n{training_utils.array_tree_to_info(train_state.params)}")

    if resuming:
        train_state = _checkpoints.restore_state(checkpoint_manager, train_state, data_loader)

    ptrain_step = jax.jit(
        functools.partial(train_step, config),
        in_shardings=(replicated_sharding, train_state_sharding, data_sharding),
        out_shardings=(train_state_sharding, replicated_sharding),
        donate_argnums=(1,),
    )

    start_step = int(train_state.step)
    pbar = tqdm.tqdm(
        range(start_step, config.num_train_steps),
        initial=start_step,
        total=config.num_train_steps,
        dynamic_ncols=True,
    )

    infos = []
    for step in pbar:
        with sharding.set_mesh(mesh):
            train_state, info = ptrain_step(train_rng, train_state, batch)
        infos.append(info)
        if step % config.log_interval == 0:
            stacked_infos = common_utils.stack_forest(infos)
            reduced_info = jax.device_get(jax.tree.map(jnp.mean, stacked_infos))
            info_str = ", ".join(f"{k}={v:.4f}" for k, v in reduced_info.items())
            pbar.write(f"Step {step}: {info_str}")
            wandb.log(reduced_info, step=step)
            infos = []
        batch = next(data_iter)

        if (step % config.save_interval == 0 and step > start_step) or step == config.num_train_steps - 1:
            _checkpoints.save_state(checkpoint_manager, train_state, data_loader, step)

    logging.info("Waiting for checkpoint manager to finish")
    checkpoint_manager.wait_until_finished()


if __name__ == "__main__":
    main(_config.cli())
