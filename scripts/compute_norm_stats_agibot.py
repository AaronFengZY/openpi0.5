import os
import json
import numpy as np
import argparse
from multiprocessing import Pool, cpu_count
from functools import partial
import tqdm

# ================= é…ç½® =================
# å¿…é¡»ä¸ action.py çš„ get_property_list ä¸¥æ ¼ä¸€è‡´
INDEX_GRIPPER = 4
INDEX_JOINT = 8
JOINT_DIM = 14
TOTAL_OUTPUT_DIM = 32

def parse_args():
    parser = argparse.ArgumentParser(description="Fast Parallel Dataset Mean/Std Compute")
    parser.add_argument("--root_dir", type=str, 
                        default="/home/v-zhifeng/HPE/v-zhifeng/agibot_beta_split_500/actions_gaussian")
    parser.add_argument("--meta_file", type=str, default="meta_data.json")
    parser.add_argument("--output_json", type=str, default="dataset_stats_mp.json")
    parser.add_argument("--workers", type=int, default=32, help="Number of parallel processes")
    return parser.parse_args()

def load_data_custom(npy_path, dim_list):
    """
    Worker è¿›ç¨‹è¯»å–å•ä¸ªæ–‡ä»¶çš„é€»è¾‘
    """
    try:
        expected_dim = sum(dim_list)
        with open(npy_path, 'rb') as f:
            raw_data = np.fromfile(f, dtype=np.float32)
        
        if raw_data.size % expected_dim != 0:
            return None
            
        T = raw_data.size // expected_dim
        matrix = raw_data.reshape(T, expected_dim)
        
        # å¿«é€Ÿåˆ‡ç‰‡æå– Joint æ•°æ®
        current_idx = 0
        for i, dim in enumerate(dim_list):
            if i == INDEX_JOINT:
                return matrix[:, current_idx : current_idx + dim]
            current_idx += dim
        return None
    except Exception:
        return None

def process_episode(args):
    """
    Worker å‡½æ•°ï¼šå¤„ç†å•ä¸ª Episodeï¼Œè¿”å›ç»Ÿè®¡é‡çš„ä¸­é—´ç»“æœ (Sum, SqSum, Count)
    å‚æ•° args æ˜¯ä¸€ä¸ª tuple: (root_dir, ep_key, dim_list)
    """
    root_dir, ep_key, dim_list = args
    npy_path = os.path.join(root_dir, ep_key, "action.npy")
    
    if not os.path.exists(npy_path):
        return None

    # åŠ è½½æ•°æ®
    joints = load_data_custom(npy_path, dim_list)
    if joints is None or joints.shape[1] != JOINT_DIM:
        return None

    # è½¬ float64 ä¿è¯ç´¯åŠ ç²¾åº¦
    joints = joints.astype(np.float64)
    T = joints.shape[0]
    
    if T < 2: return None

    # --- 1. State Stats (Joints Abs) ---
    s_sum = np.sum(joints, axis=0)
    s_sq_sum = np.sum(joints ** 2, axis=0)
    s_count = T

    # --- 2. Action Stats (Joints Delta) ---
    joint_delta = joints[1:] - joints[:-1]
    a_sum = np.sum(joint_delta, axis=0)
    a_sq_sum = np.sum(joint_delta ** 2, axis=0)
    a_count = T - 1

    return (s_sum, s_sq_sum, s_count, a_sum, a_sq_sum, a_count)

def main():
    args = parse_args()
    
    # 1. åŠ è½½ Meta Data
    meta_path = os.path.join(args.root_dir, args.meta_file)
    print(f"ğŸ“– Loading meta data from {meta_path}...")
    with open(meta_path, 'r') as f:
        meta_data = json.load(f)
    
    episodes = list(meta_data.keys())
    total_episodes = len(episodes)
    
    # 2. å‡†å¤‡ä»»åŠ¡å‚æ•°åˆ—è¡¨
    # å°†ä¸éœ€è¦çš„å¤§å­—å…¸è§£è€¦ï¼Œåªä¼ å¿…è¦å‚æ•°ç»™ worker
    tasks = []
    for ep_key in episodes:
        tasks.append((args.root_dir, ep_key, meta_data[ep_key]["dim_list"]))
    
    # 3. ç¡®å®šè¿›ç¨‹æ•°
    # å¦‚æœæ²¡æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•° - 2 (ç•™ç‚¹ä½™é‡)
    num_workers = args.workers if args.workers > 0 else max(1, cpu_count() - 2)
    print(f"ğŸš€ Starting multiprocessing with {num_workers} workers for {total_episodes} episodes...")

    # 4. å…¨å±€ç´¯åŠ å™¨ (float64)
    total_state_sum = np.zeros(JOINT_DIM, dtype=np.float64)
    total_state_sq_sum = np.zeros(JOINT_DIM, dtype=np.float64)
    total_state_count = 0

    total_action_sum = np.zeros(JOINT_DIM, dtype=np.float64)
    total_action_sq_sum = np.zeros(JOINT_DIM, dtype=np.float64)
    total_action_count = 0

    valid_files = 0

    # 5. å¹¶è¡Œæ‰§è¡Œ
    # chunksize ç¨å¾®è®¾å¤§ä¸€ç‚¹ (ä¾‹å¦‚ 100)ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡å¼€é”€
    with Pool(processes=num_workers) as pool:
        # ä½¿ç”¨ imap_unordered è·å–ç»“æœï¼Œé…åˆ tqdm æ˜¾ç¤ºè¿›åº¦
        results = list(tqdm.tqdm(pool.imap_unordered(process_episode, tasks, chunksize=50), total=total_episodes))

    print("ğŸ“Š Aggregating results...")
    
    # 6. æ±‡æ€»ç»“æœ
    for res in results:
        if res is None:
            continue
            
        (s_sum, s_sq_sum, s_cnt, a_sum, a_sq_sum, a_cnt) = res
        
        total_state_sum += s_sum
        total_state_sq_sum += s_sq_sum
        total_state_count += s_cnt
        
        total_action_sum += a_sum
        total_action_sq_sum += a_sq_sum
        total_action_count += a_cnt
        
        valid_files += 1

    print(f"âœ… Processed {valid_files} valid files.")

    if total_state_count == 0:
        print("âŒ No valid data found.")
        return

    # 7. è®¡ç®— Mean/Std
    state_mean = total_state_sum / total_state_count
    state_std = np.sqrt((total_state_sq_sum / total_state_count) - (state_mean ** 2) + 1e-8)

    action_mean = total_action_sum / total_action_count
    action_std = np.sqrt((total_action_sq_sum / total_action_count) - (action_mean ** 2) + 1e-8)

    # 8. æ ¼å¼åŒ–è¾“å‡º (Padding)
    def format_output(mean_arr, std_arr):
        final_mean = np.zeros(TOTAL_OUTPUT_DIM, dtype=np.float32)
        final_std = np.ones(TOTAL_OUTPUT_DIM, dtype=np.float32)
        
        # å¡«å…¥è®¡ç®—ç»“æœï¼Œè½¬å› float32 å­˜ JSON
        final_mean[:JOINT_DIM] = mean_arr.astype(np.float32)
        final_std[:JOINT_DIM] = std_arr.astype(np.float32)
        
        return final_mean.tolist(), final_std.tolist()

    st_mean_list, st_std_list = format_output(state_mean, state_std)
    act_mean_list, act_std_list = format_output(action_mean, action_std)

    stats_dict = {
        "norm_stats": {
            "state": {
                "mean": st_mean_list,
                "std":  st_std_list
            },
            "actions": {
                "mean": act_mean_list,
                "std":  act_std_list
            }
        }
    }

    save_path = os.path.join(args.root_dir, args.output_json)
    with open(save_path, 'w') as f:
        json.dump(stats_dict, f, indent=2)

    print("\n" + "="*50)
    print(f"âœ… Fast Stats saved to: {save_path}")
    print(f"   Workers used: {num_workers}")
    print(f"   Joint Mean (First 5): {state_mean[:5]}")
    print("="*50)

if __name__ == "__main__":
    main()